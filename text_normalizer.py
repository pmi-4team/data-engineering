import re
import redis
from kiwipiepy import Kiwi


class TextNormalizer:
    def __init__(self, redis_host='localhost', redis_port=6379, redis_db=0):
        """í…ìŠ¤íŠ¸ ì •ê·œí™” í´ë˜ìŠ¤"""
        print("âœ… TextNormalizer: Redis ì—°ê²° ì‹œë„ ì¤‘...")
        self.redis = redis.StrictRedis(
            host=redis_host, port=redis_port, db=redis_db, decode_responses=True
        )
        print("âœ… TextNormalizer: Redis ì—°ê²° ì„±ê³µ")

        print("ğŸ”§ TextNormalizer: Kiwipiepy ì´ˆê¸°í™” ì¤‘...")
        self.kiwi = Kiwi()
        print("âœ… TextNormalizer: Kiwipiepy ì´ˆê¸°í™” ì™„ë£Œ")

        # Redis í•´ì‹œ í‚¤ (sync_dict_to_redis.pyì™€ ë™ì¼)
        self.hash_typo = "typo_rules"
        self.hash_synonym = "synonym_rules"

        # ê·œì¹™ ì €ì¥ìš©
        self.typo_patterns = []
        self.synonym_patterns = []

        # ê·œì¹™ ë¡œë“œ
        self.load_rules_from_redis()

    # -------------------------------
    # 1ï¸âƒ£ Redisì—ì„œ ê·œì¹™ ë¡œë“œ
    # -------------------------------
    def load_rules_from_redis(self):
        print("\nğŸ“¥ TextNormalizer: Redisì—ì„œ ê·œì¹™ ë¡œë“œ ì¤‘...")

        typo_rules = self.redis.hgetall(self.hash_typo)
        synonym_rules = self.redis.hgetall(self.hash_synonym)

        print(f"  - Typo ê·œì¹™: {len(typo_rules)}ê°œ")
        print(f"  - Synonym ê·œì¹™: {len(synonym_rules)}ê°œ")

        self.typo_patterns = [(k, v) for k, v in typo_rules.items() if k and v and k != v]
        self.synonym_patterns = sorted(
            [(k, v) for k, v in synonym_rules.items() if k and v and k != v],
            key=lambda x: len(x[0]),
            reverse=True
        )

        print(f"  [Typo] ê·œì¹™ ë¡œë“œ ì™„ë£Œ ({len(self.typo_patterns)}ê°œ)")
        print(f"  [Synonym] ê·œì¹™ ë¡œë“œ ì™„ë£Œ ({len(self.synonym_patterns)}ê°œ)")
        print("âœ… TextNormalizer: ê·œì¹™ ë¡œë“œ ì™„ë£Œ\n")

    # -------------------------------
    # 2ï¸âƒ£ ì „ì²˜ë¦¬
    # -------------------------------
    def preprocess(self, text: str) -> str:
        text = text.strip()
        text = text.replace("'", "").replace('"', "")
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    # -------------------------------
    # 3ï¸âƒ£ ì¹˜í™˜ (ì¤‘ë³µ/ê²¹ì¹¨/ì¬ì ìš© ë°©ì§€)
    # -------------------------------
    def apply_replacements(self, text: str, rules, stage_name: str):
        applied = []
        used_rules = set()

        for term_from, term_to in rules:
            if (term_from, term_to) in used_rules:
                continue

            # íŒ¨í„´ ì •ì˜: ë‹¨ì–´ ê²½ê³„, ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€
            pattern = re.compile(rf"(?<!\w){re.escape(term_from)}(?!\w)")
            if not pattern.search(text):
                continue

            # êµì²´ ìˆ˜í–‰
            new_text, count = pattern.subn(term_to, text)
            if count > 0:
                # ğŸ”’ term_toê°€ í¬í•¨ëœ ë¶€ë¶„ì€ ë‹¤ì‹œ êµì²´í•˜ì§€ ì•Šë„ë¡
                if term_to in text:
                    continue

                text = new_text
                applied.append((term_from, term_to))
                used_rules.add((term_from, term_to))

        # í›„ì²˜ë¦¬: â€œì´ìƒ ì´ìƒâ€ / â€œë§Œì› ë§Œì›â€ ê°™ì€ ì¤‘ë³µ ì œê±°
        text = re.sub(r"(\b\w+\b)(\s+\1)+", r"\1", text)
        text = re.sub(r"\s{2,}", " ", text).strip()

        if applied:
            print(f"2ï¸âƒ£ {stage_name} ì¹˜í™˜ ({len(applied)}ê±´): {applied[:5]}")
        return text, applied

    # -------------------------------
    # 4ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„ + ë¬¸ì¥ ë³µì›
    # -------------------------------
    def tokenize_and_normalize(self, text: str):
        sentences = self.kiwi.split_into_sents(text)
        restored_text = " ".join([s.text for s in sentences]).strip()
        tokens = self.kiwi.tokenize(text)
        token_tuples = [(t.form, t.tag) for t in tokens]
        print(f"3ï¸âƒ£ í† í°í™” ê²°ê³¼: {token_tuples}")
        return restored_text

    # -------------------------------
    # 5ï¸âƒ£ ì „ì²´ ì •ì œ íŒŒì´í”„ë¼ì¸
    # -------------------------------
    def normalize(self, text: str, verbose: bool = True):
        """ì „ì²´ ì •ì œ íŒŒì´í”„ë¼ì¸"""
        original_text = text
        if verbose:
            print("============================================================")
            print(f"ì›ë³¸ (Raw): {original_text}")
            print("============================================================")

        # 0ï¸âƒ£ ì „ì²˜ë¦¬
        text = self.preprocess(text)
        if verbose:
            print(f"0ï¸âƒ£ ì „ì²˜ë¦¬: {text}")

        # 1ï¸âƒ£ Typo êµì •
        if self.typo_patterns:
            text, _ = self.apply_replacements(text, self.typo_patterns, "Typo")

        # 2ï¸âƒ£ Synonym êµì •
        if self.synonym_patterns:
            text, _ = self.apply_replacements(text, self.synonym_patterns, "Synonym")

        # 3ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„ ë° ë³µì›
        text = self.tokenize_and_normalize(text)

        if verbose:
            print(f"âœ… ìµœì¢… ê²°ê³¼: {text}")
            print("============================================================")

        return text
